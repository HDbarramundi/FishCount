 # Fish Counting 
 If you want to dig little bit more in the theory behind this algorithms and how it is really working. In this section I am bit trying to explain, there could be some misleading
 so, anyone can correct if they find it wrong. YOU ARE MOST WELCOME TO CONTRIBUTE.
 As we are using, YOLO model to first detect and count the fish. In future we will do more than that. 
 I assume you have gone through $\textbf{\textcolor{purple}{README.md}}$ file. So, that I do not have to explain steps here.

## What is Machine Learning ?


$$
CO_2 = 1.22 \times (Alkalinity) \times 10^{(-43.6977 - 0.0129037 \times (Salinity) + 1.364 \times 10^{-4} \times (Salinity)^2 + 2885.378 / (273.15 + Temp) + 7.045159 \times \log(273.15 + Temp)) - pH}
$$



# YOLO (You Only Look Once) - Comprehensive Guide

YOLO (You Only Look Once) is a state-of-the-art real-time object detection framework. It processes an image in a single forward pass through a deep neural network, making it highly efficient compared to traditional object detection methods.

## **1. Theoretical Foundation**

### **1.1 Object Detection as a Regression Problem**

YOLO reformulates object detection as a single regression problem instead of using region proposal methods (e.g., R-CNN). This allows it to predict bounding boxes and class probabilities directly from an image.

Mathematically, YOLO predicts:
- **Bounding Box Coordinates**: \( x, y, w, h \)
- **Object Confidence Score**: \( C \)
- **Class Probabilities**: \( P(c_i) \)

The final score for each bounding box is calculated as:
\[
Score = C \times P(c_i)
\]

### **1.2 Grid Cell Division**

The input image is divided into an \( S \times S \) grid. Each grid cell predicts \( B \) bounding boxes and their associated confidence scores. Each bounding box is represented as:
\[
(x, y, w, h, C)
\]

Where:
- \( x, y \) are relative to the grid cell.
- \( w, h \) are normalized relative to the image dimensions.
- \( C \) represents the confidence score, incorporating Intersection over Union (IoU) between the predicted and ground truth box.

### **1.3 Loss Function**

The YOLO loss function consists of three main components:

\[
\mathcal{L} = \lambda_{coord} \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^{obj} \Big[ (x_i - \hat{x}_i)^2 + (y_i - \hat{y}_i)^2 \Big] \\
+ \lambda_{coord} \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^{obj} \Big[ (\sqrt{w_i} - \sqrt{\hat{w}_i})^2 + (\sqrt{h_i} - \sqrt{\hat{h}_i})^2 \Big] \\
+ \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^{obj} (C_i - \hat{C}_i)^2 \\
+ \lambda_{noobj} \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^{noobj} (C_i - \hat{C}_i)^2 \\
+ \sum_{i=0}^{S^2} \mathbb{1}_{i}^{obj} \sum_{c \in classes} (p_i(c) - \hat{p}_i(c))^2
\]

Where:
- \( \mathbb{1}_{ij}^{obj} \) indicates if object appears in the cell.
- \( \lambda_{coord} \) and \( \lambda_{noobj} \) are weight parameters.

---

## **2. YOLO Architecture**

YOLO models utilize CNN-based backbones for feature extraction. The most commonly used architectures include:

| Version  | Backbone          | Key Improvements |
|----------|------------------|------------------|
| YOLOv1   | Darknet-19       | First iteration |
| YOLOv2   | Darknet-19       | Anchor boxes, batch normalization |
| YOLOv3   | Darknet-53       | Multi-scale detection |
| YOLOv4   | CSPDarknet53     | Mish activation, PANet |
| YOLOv5   | CSPDarknet53     | Focus layer, AutoAnchor tuning |
| YOLOv6-8 | Transformer-CNN  | Efficient layer aggregation |
| YOLOv11  | C3K2 Blocks      | Improved attention mechanisms |

---

## **3. YOLO Code Implementation (YOLOv5)**

Below is a simple implementation of YOLOv5 using PyTorch:

```python
import torch
from yolov5 import detect

# Load pre-trained YOLOv5 model
model = torch.hub.load('ultralytics/yolov5', 'yolov5s')

# Perform inference on an image
image_path = 'image.jpg'
results = model(image_path)

# Display results
results.show()
```

---

## **4. Key Parameters in YOLO Models**

- **Confidence Threshold (conf-thres):** Filters out detections below a certain probability.
- **IoU Threshold (iou-thres):** Determines overlap for non-maximum suppression.
- **Anchor Boxes:** Predefined sizes used for bounding box regression.
- **Batch Size:** Number of images processed per training step.
- **Learning Rate:** Controls weight updates during training.

---

## **5. Conclusion**

YOLO remains a powerful and efficient object detection framework due to its speed and accuracy. It continues to evolve with improved architectures and optimizations, making it suitable for real-world applications such as surveillance, autonomous driving, and medical imaging.

















 # YOLO (You Only Look Once) - Object Detection

YOLO is a real-time object detection algorithm that processes an image in a single pass through a neural network. Unlike traditional object detection methods, which apply classifiers to different regions of an image, YOLO treats detection as a single regression problem, predicting bounding boxes and class probabilities simultaneously.

## **How YOLO Works:**

1. **Input Image Splitting:**  
   - The input image is divided into an \( S \times S \) grid (e.g., 13Ã—13 for YOLOv3 at 416Ã—416 resolution).
   - Each grid cell is responsible for detecting objects whose center falls within it.

2. **Bounding Box Predictions:**  
   - Each grid cell predicts a fixed number of bounding boxes (B), typically 2â€“5.
   - Each bounding box includes:
     - \( x, y \) (coordinates relative to the grid cell)
     - \( w, h \) (width and height relative to the image)
     - Confidence score (probability of an object in the box Ã— IoU with the ground truth box).

3. **Class Predictions:**  
   - Each grid cell also predicts class probabilities for detected objects.
   - The final score for each bounding box is **confidence Ã— class probability**.

4. **Non-Maximum Suppression (NMS):**  
   - Since multiple boxes may detect the same object, NMS filters out redundant predictions.
   - The box with the highest confidence is kept, and overlapping boxes (IoU > threshold) are removed.

## **YOLO Versions:**

- **YOLOv1 (2015):** Introduced single-shot detection with real-time performance.
- **YOLOv2 (2016):** Improved accuracy and speed with batch normalization and anchor boxes.
- **YOLOv3 (2018):** Added multi-scale predictions and Darknet-53 backbone.
- **YOLOv4 (2020):** Optimized speed and accuracy with CSPDarknet.
- **YOLOv5 (2020, Ultralytics):** Not an official continuation but widely used, optimized for PyTorch.
- **YOLOv6, YOLOv7, YOLOv8:** Further refinements in efficiency and accuracy.

## **Why YOLO?**
âœ… **Fast** â€“ Real-time processing (~30â€“150 FPS).  
âœ… **Accurate** â€“ Good balance of speed and precision.  
âœ… **End-to-End Learning** â€“ Entire image processed in one forward pass.

Would you like help implementing YOLO for a specific task? ðŸš€

 ## HOW different tracks are important and how to use them?
 ## How Length and Weight of fish are calculated?
 ### How we save the file in computer especially export in excel file 


